{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf, RDD, SparkContext, SparkSession class into program\n",
    "from pyspark import SparkConf\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Using all the processor \n",
    "master = \"local[*]\"\n",
    "# Initiallizing app name\n",
    "app_name = \"Task 1\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# instantiating a SparkContext\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import all the “Units” csv files from 2015-2019 into a single RDD.**\n",
    "\n",
    "**Import all the “Crashes” csv files from 2015-2019 into a single RDD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all the Unit and Crash data into single RDD respectively\n",
    "Unit_RDD = sc.textFile('data/*_Units.csv')\n",
    "Crash_RDD = sc.textFile('data/*_Crash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For both Units and Crashes, use csv.reader to parse each row as a nested list in the\n",
    "RDD, and remove the header rows and display the total count and first 10 records.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting header row from the RDD\n",
    "UnitHeader = Unit_RDD.first()\n",
    "CrashHeader = Crash_RDD.first()\n",
    "# removing the header row for both Unit and Crash RDD object\n",
    "Unit_RDD = Unit_RDD.filter(lambda x: x != UnitHeader)\n",
    "Crash_RDD = Crash_RDD.filter(lambda x: x != CrashHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count for Unit Data:  153854\n",
      "Total Count for Crash Data:  72006 \n",
      "\n",
      "First 10 records for Unit Data \n",
      " [['2016-1-15/08/2019', '01', '0', 'SA', 'OMNIBUS', '2011', 'North', 'Male', '056', 'SA', 'HR', 'Full', 'Not Towing', 'Straight Ahead', '010', '5121', '', ''], ['2016-1-15/08/2019', '02', '1', '', 'Pedestrian on Road', '', 'East', 'Male', '072', '', '', '', '', 'Walking on Road', '', '5084', '', ''], ['2016-2-15/08/2019', '01', '0', 'SA', 'Motor Cars - Sedan', '2004', 'Unknown', 'Female', '023', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5087', '', ''], ['2016-2-15/08/2019', '02', '0', 'SA', 'Station Wagon', '2008', 'Unknown', 'Male', '040', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5084', '', ''], ['2016-3-15/08/2019', '01', '0', 'SA', 'RIGID TRUCK LGE GE 4.5T', '1990', 'South', 'Unknown', 'XXX', 'SA', 'MR', 'Provisional 2', 'Not Towing', 'Straight Ahead', '001', '5115', '', ''], ['2016-3-15/08/2019', '02', '0', 'SA', 'Panel Van', '2013', 'South', 'Male', '023', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5110', '', ''], ['2016-4-15/08/2019', '01', '0', 'SA', 'Station Wagon', '2002', 'East', 'Female', '033', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5169', '', ''], ['2016-4-15/08/2019', '02', '0', 'UNKNOWN', 'Other Defined Special Vehicle', 'XXXX', 'North', 'Unknown', 'XXX', 'UNKNOWN', 'XX', 'Unknown', 'Unknown', 'Reversing', '001', 'XXXX', '', ''], ['2016-5-15/08/2019', '01', '1', 'SA', 'Motor Cars - Sedan', '1997', 'South East', 'Male', '042', 'SA', 'C ', 'Full', 'Not Towing', 'Right Turn', '001', 'XXXX', '', ''], ['2016-5-15/08/2019', '02', '2', 'SA', 'Utility', '2015', 'North East', 'Male', '059', 'SA', 'MC', 'Full', 'Not Towing', 'Straight Ahead', '002', '5114', '', '']]\n",
      "\n",
      "First 10 records for Crash Data \n",
      " [['2019-1-8/07/2020', '2 Metropolitan', 'HAMPSTEAD GARDENS', '5086', 'CITY OF PORT ADELAIDE ENFIELD', '2', '0', '0', '0', '0', '2019', 'June', 'Wednesday', '11:15 am', '060', 'Cross Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Right Angle', '01', 'Driver Rider', '1: PDO', 'Give Way Sign', '', '', '1331810.03', '1676603.26', '13318101676603'], ['2019-2-8/07/2020', '2 Metropolitan', 'DRY CREEK', '5094', 'CITY OF SALISBURY', '2', '0', '0', '0', '0', '2019', 'January', 'Tuesday', '12:49 am', '090', 'Divided Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Rear End', '02', 'Driver Rider', '1: PDO', 'No Control', '', '', '1328376.2', '1682942.63', '13283761682943'], ['2019-3-8/07/2020', '2 Metropolitan', 'MILE END', '5031', 'CITY OF WEST TORRENS', '2', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '12:00 am', '060', 'Divided Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Hit Pedestrian', '01', 'Driver Rider', '2: MI', 'No Control', '', '', '1325819.68', '1670994.26', '13258201670994'], ['2019-4-8/07/2020', '2 Metropolitan', 'PARALOWIE', '5108', 'CITY OF SALISBURY', '2', '1', '0', '1', '0', '2019', 'January', 'Tuesday', '12:05 am', '050', 'Not Divided', 'CURVED', ' VIEW OPEN', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Hit Fixed Object', '01', 'Driver Rider', '3: SI', 'No Control', '', '', '1328320.6', '1690237.08', '13283211690237'], ['2019-5-8/07/2020', '2 Metropolitan', 'MOUNT BARKER', '5251', 'DC MT.BARKER.                 ', '2', '0', '0', '0', '0', '2019', 'January', 'Tuesday', '05:15 am', '110', 'Divided Road', 'Straight road', 'Slope', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Hit Animal', '02', 'Animal', '1: PDO', 'No Control', '', '', '1353279.99', '1655645.15', '13532801655645'], ['2019-6-8/07/2020', '2 Metropolitan', 'TORRENSVILLE', '5031', 'CITY OF WEST TORRENS', '2', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '07:00 am', '050', 'Divided Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Hit Fixed Object', '01', 'Driver Rider', '2: MI', 'No Control', '', '', '1324652.75', '1672027.64', '13246531672028'], ['2019-7-8/07/2020', '2 Metropolitan', 'BEDFORD PARK', '5042', 'CC MITCHAM.                   ', '2', '3', '0', '0', '3', '2019', 'January', 'Tuesday', '09:40 am', '050', 'Cross Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Right Angle', '02', 'Driver Rider', '2: MI', 'Traffic Signals', '', '', '1325156.2', '1660414.38', '13251561660414'], ['2019-8-8/07/2020', '3 Country', 'WYE', '5291', 'DISTRICT COUNCIL OF GRANT', '1', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '12:15 pm', '110', 'Not Divided', 'CURVED', ' VIEW OPEN', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Roll Over', '01', 'Driver Rider', '2: MI', 'No Control', '', '', '1517347.8', '1321979.33', '15173481321979'], ['2019-9-8/07/2020', '3 Country', 'MOUNT GAMBIER', '5290', 'CC MT.GAMBIER.                ', '1', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '11:45 am', '050', 'T-Junction', 'Straight road', 'Bottom of Hill', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Roll Over', '01', 'Driver Rider', '2: MI', 'Stop Sign', '', '', '1510220.05', '1340472.35', '15102201340472'], ['2019-10-8/07/2020', '3 Country', 'OVERLAND CORNER', '5330', 'THE BERRI BARMERA COUNCIL', '1', '1', '0', '1', '0', '2019', 'January', 'Tuesday', '12:30 pm', '080', 'Not Divided', 'CURVED', ' VIEW OPEN', 'Slope', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Roll Over', '01', 'Driver Rider', '3: SI', 'No Control', '', '', '1492599.9', '1749395.71', '14926001749396']]\n"
     ]
    }
   ],
   "source": [
    "# creating parser to prse the data\n",
    "def parser(line):\n",
    "    lines = line.splitlines()\n",
    "    read = csv.reader(lines, delimiter=',')\n",
    "    for row in read:\n",
    "        x = ','.join(row)\n",
    "    return x.split(',')\n",
    "\n",
    "# Parsing unit and Crash data\n",
    "Unit_RDD1 = Unit_RDD.map(parser)\n",
    "Crash_RDD1 = Crash_RDD.map(parser)\n",
    "\n",
    "print(\"Total Count for Unit Data: \",Unit_RDD1.count())\n",
    "print(\"Total Count for Crash Data: \",Crash_RDD1.count(), \"\\n\")\n",
    "\n",
    "print(\"First 10 records for Unit Data \\n\",Unit_RDD1.take(10))\n",
    "print(\"\\nFirst 10 records for Crash Data \\n\", Crash_RDD1.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Data Partitioning in RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many partitions do the above RDDs have? How is the data in these RDDs\n",
    "partitioned by default, when we do not explicitly specify any partitioning strategy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans: By default there are 5 partitions. Data in these partitions are almost equally distributed but with some partitions having more data than others**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions for Unit:  5\n",
      "Default partitions for Crash:  5\n",
      "\n",
      "Partition for Unit data\n",
      "Partition 0 No of records: 35861\n",
      "Partition 1 No of records: 28163\n",
      "Partition 2 No of records: 33084\n",
      "Partition 3 No of records: 27713\n",
      "Partition 4 No of records: 29033\n",
      "\n",
      "Partition for Crash data\n",
      "Partition 0 No of records: 12964\n",
      "Partition 1 No of records: 16775\n",
      "Partition 2 No of records: 13237\n",
      "Partition 3 No of records: 13599\n",
      "Partition 4 No of records: 15431\n"
     ]
    }
   ],
   "source": [
    "# Printing default partitions in data\n",
    "print('Default partitions for Unit: ',Unit_RDD1.getNumPartitions())\n",
    "print('Default partitions for Crash: ',Crash_RDD1.getNumPartitions())\n",
    "\n",
    "# collecting partitions for Unit data\n",
    "Unit_partition = Unit_RDD1.glom().collect()\n",
    "\n",
    "print(\"\\nPartition for Unit data\")\n",
    "for index, part in enumerate(Unit_partition):\n",
    "    if len(part) > 0:\n",
    "        print(\"Partition\", index, \"No of records:\", len(part))\n",
    "\n",
    "# collecting partitions for Crash data\n",
    "Crash_partition = Crash_RDD1.glom().collect()\n",
    "\n",
    "print(\"\\nPartition for Crash data\")\n",
    "for index, part in enumerate(Crash_partition):\n",
    "    if len(part) > 0:\n",
    "        print(\"Partition\", index, \"No of records:\", len(part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Key Value Pair RDD with Lic State as the key and rest of the column as\n",
    "value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Key value pair\n",
    "def keyValue(li):\n",
    "    x = li.pop(9)\n",
    "    return (x, li[:])\n",
    "\n",
    "# Creating key value pair RDD with Lic State as key\n",
    "Unit_RDD_keyValue = Unit_RDD1.map(keyValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write the code to implement this partitioning in RDD using appropriate\n",
    "partitioning functions.**\n",
    "\n",
    "**Write the code to print the number of records in each partition. What does it\n",
    "tell about the data skewness?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans: Lic State equals to 'SA' has 109683 records and other states have 44175 records. We can clearly see the data skweness as most of the records belongs to 'SA' state. Since we have done partitining based on state we have data skweness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0 No of records: 109684\n",
      "Partition 1 No of records: 44170\n"
     ]
    }
   ],
   "source": [
    "NoOfPartition = 2\n",
    "\n",
    "# partitining function based on Lic State equals to 'SA' \n",
    "def hash_partition(key):\n",
    "    if key == 'SA':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Creating two partitions based on Lic State\n",
    "Unit_RDD2 = Unit_RDD_keyValue.partitionBy(NoOfPartition,hash_partition)\n",
    "\n",
    "partition = Unit_RDD2.glom().collect()\n",
    "\n",
    "for index, part in enumerate(partition):\n",
    "    if len(part) > 0:\n",
    "        print(\"Partition\", index, \"No of records:\", len(part))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Query/Analysis ( 10% )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the average age of male and female drivers separately.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age for Males:  40.98\n",
      "\n",
      "Mean age for Females 40.39\n"
     ]
    }
   ],
   "source": [
    "# Filtering RDD based on Male and Female\n",
    "Age_Male = Unit_RDD2.filter(lambda x: (x[1][7]=='Male') and (x[1][8]!='XXX'))\n",
    "Age_Female = Unit_RDD2.filter(lambda x: x[1][7]=='Female' and (x[1][8]!='XXX'))\n",
    "\n",
    "# Function to calculate Mean age\n",
    "def MeanAge(data):\n",
    "    age = data.collect()\n",
    "    i = 0\n",
    "    total = 0\n",
    "    for value in age:\n",
    "        total += int(value[1][8])\n",
    "        i+=1\n",
    "    meanAge = total/i\n",
    "    return meanAge\n",
    "\n",
    "print(\"Mean age for Males: \", round(MeanAge(Age_Male),2))\n",
    "print(\"\\nMean age for Females\", round(MeanAge(Age_Female),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the oldest and the newest vehicle year involved in the accident? Display the\n",
    "Registration State, Year and Unit type of the vehicle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest Vehicle:\n",
      "['SA', 'Station Wagon', '2019']\n",
      "\n",
      "Oldest Vehicle:\n",
      "['SA', 'Motor Cycle', '1900']\n"
     ]
    }
   ],
   "source": [
    "# Removing unwanted data rows\n",
    "cleanData = Unit_RDD2.filter(lambda x: (x[1][5]!='XXXX') and x[1][5]!='')\n",
    "# finding max year\n",
    "result_max_year = cleanData.max(key=lambda x: int(x[1][5]))\n",
    "# finding min year\n",
    "result_min_year = cleanData.min(key=lambda x: int(x[1][5]))\n",
    "# filtering Registration State, Year and Unit type\n",
    "result_max_year = result_max_year[1][3:6]\n",
    "result_min_year = result_min_year[1][3:6]\n",
    "\n",
    "print(\"Newest Vehicle:\")\n",
    "print(result_max_year)\n",
    "print(\"\\nOldest Vehicle:\")\n",
    "print(result_min_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Working with DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all Unit data in dataframe\n",
    "df_Units = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('data/*Units.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all Crash data in dataframe\n",
    "df_Crash = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('data/*Crash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the schema of the final two dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Schema\n",
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Unit No: string (nullable = true)\n",
      " |-- No Of Cas: string (nullable = true)\n",
      " |-- Veh Reg State: string (nullable = true)\n",
      " |-- Unit Type: string (nullable = true)\n",
      " |-- Veh Year: string (nullable = true)\n",
      " |-- Direction Of Travel: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Lic State: string (nullable = true)\n",
      " |-- Licence Class: string (nullable = true)\n",
      " |-- Licence Type: string (nullable = true)\n",
      " |-- Towing: string (nullable = true)\n",
      " |-- Unit Movement: string (nullable = true)\n",
      " |-- Number Occupants: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- Rollover: string (nullable = true)\n",
      " |-- Fire: string (nullable = true)\n",
      "\n",
      "\n",
      "Crash Schema\n",
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Stats Area: string (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- LGA Name: string (nullable = true)\n",
      " |-- Total Units: string (nullable = true)\n",
      " |-- Total Cas: string (nullable = true)\n",
      " |-- Total Fats: string (nullable = true)\n",
      " |-- Total SI: string (nullable = true)\n",
      " |-- Total MI: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area Speed: string (nullable = true)\n",
      " |-- Position Type: string (nullable = true)\n",
      " |-- Horizontal Align: string (nullable = true)\n",
      " |-- Vertical Align: string (nullable = true)\n",
      " |-- Other Feat: string (nullable = true)\n",
      " |-- Road Surface: string (nullable = true)\n",
      " |-- Moisture Cond: string (nullable = true)\n",
      " |-- Weather Cond: string (nullable = true)\n",
      " |-- DayNight: string (nullable = true)\n",
      " |-- Crash Type: string (nullable = true)\n",
      " |-- Unit Resp: string (nullable = true)\n",
      " |-- Entity Code: string (nullable = true)\n",
      " |-- CSEF Severity: string (nullable = true)\n",
      " |-- Traffic Ctrls: string (nullable = true)\n",
      " |-- DUI Involved: string (nullable = true)\n",
      " |-- Drugs Involved: string (nullable = true)\n",
      " |-- ACCLOC_X: string (nullable = true)\n",
      " |-- ACCLOC_Y: string (nullable = true)\n",
      " |-- UNIQUE_LOC: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing Schema for both Unit and Crash Data\n",
    "print(\"Unit Schema\")\n",
    "df_Units.printSchema()\n",
    "print(\"\\nCrash Schema\")\n",
    "df_Crash.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find all the crash events in Adelaide where the total number of casualties in the event\n",
    "is more than 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|Stats Area|  Suburb|Postcode|        LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|     Day|    Time|Area Speed|Position Type|Horizontal Align|Vertical Align|          Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight|    Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2018-601-17/01/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          8|        4|         0|       2|       2|2018| January|  Sunday|09:12 pm|       050|  Not Divided|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|   Night|Hit Pedestrian|       01|Driver Rider|        3: SI|     No Control|        null|          null|1329806.36|1670224.76|13298061670225|\n",
      "|2017-1613-15/08/2019|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          2|        4|         0|       0|       4|2017|February|Saturday|04:00 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|Traffic Signals|        null|          null|1327951.24|1669556.92|13279511669557|\n",
      "|2017-12182-15/08/...|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          6|        5|         0|       1|       4|2017|December|Saturday|04:08 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Wet| Not Raining|Daylight|Hit Pedestrian|       01|Driver Rider|        3: SI|Traffic Signals|        null|          null| 1329016.2|1670995.07|13290161670995|\n",
      "|2019-10404-8/07/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          4|        6|         0|       0|       6|2019| October|  Monday|08:20 am|       060| Divided Road|   Straight road|         Level|Driveway or Entrance|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|     No Control|        null|          null|1327088.72|1670880.07|13270891670880|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting Total cas data to int\n",
    "df_Crash['Total Cas'].cast(IntegerType())\n",
    "# filtering data based on ADELAIDE suburd and total cas greater than 3\n",
    "df_Adelaide = df_Crash[(df_Crash['Suburb'] == 'ADELAIDE') & (df_Crash['Total Cas'] > 3)]\n",
    "df_Adelaide.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display 10 crash events with highest casualties .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|          Crash Type|Total casualties|\n",
      "+--------------------+----------------+\n",
      "|            Rear End|          8605.0|\n",
      "|         Right Angle|          6555.0|\n",
      "|    Hit Fixed Object|          4502.0|\n",
      "|          Right Turn|          3120.0|\n",
      "|           Roll Over|          2279.0|\n",
      "|          Side Swipe|          1992.0|\n",
      "|      Hit Pedestrian|          1517.0|\n",
      "|             Head On|          1224.0|\n",
      "|  Hit Parked Vehicle|          1133.0|\n",
      "|Left Road - Out o...|           244.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total casualties based on crash type\n",
    "Top10_Casualties = df_Crash.groupBy('Crash Type').agg(F.sum('Total Cas').alias('Total casualties')).sort('Total casualties', ascending=False)\n",
    "# Displaying top 10 crash\n",
    "Top10_Casualties.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of fatalities for each crash type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|          Crash Type|fatalities|\n",
      "+--------------------+----------+\n",
      "|           Roll Over|      57.0|\n",
      "|  Hit Object on Road|       2.0|\n",
      "|      Hit Pedestrian|      70.0|\n",
      "|    Hit Fixed Object|     152.0|\n",
      "|               Other|       2.0|\n",
      "|          Side Swipe|      20.0|\n",
      "|             Head On|      86.0|\n",
      "|  Hit Parked Vehicle|       9.0|\n",
      "|          Right Turn|      18.0|\n",
      "|            Rear End|      16.0|\n",
      "|          Hit Animal|       4.0|\n",
      "|Left Road - Out o...|       1.0|\n",
      "|         Right Angle|      45.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total fatalities based on crash type\n",
    "df_CrashType = df_Crash.groupBy('Crash Type').agg(F.sum('Total Fats').alias('fatalities'))\n",
    "\n",
    "# Displaying fatalities\n",
    "df_CrashType.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of casualties for each suburb when the vehicle was driven by an\n",
    "unlicensed driver. You are required to display the name of the suburb and the total\n",
    "number of casualties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|         Suburb|casualties|\n",
      "+---------------+----------+\n",
      "|       ADELAIDE|      19.0|\n",
      "|      SALISBURY|      18.0|\n",
      "|      DRY CREEK|      18.0|\n",
      "| SALISBURY EAST|      16.0|\n",
      "|       PROSPECT|      14.0|\n",
      "| NORTH ADELAIDE|      13.0|\n",
      "|        ENFIELD|      12.0|\n",
      "|   ANDREWS FARM|      12.0|\n",
      "|     INGLE FARM|      11.0|\n",
      "|   BEDFORD PARK|      11.0|\n",
      "|SALISBURY SOUTH|      11.0|\n",
      "|SALISBURY DOWNS|      11.0|\n",
      "|     MUNNO PARA|      10.0|\n",
      "|SALISBURY PLAIN|      10.0|\n",
      "|  MORPHETT VALE|      10.0|\n",
      "|   MOUNT BARKER|      10.0|\n",
      "|         BURTON|      10.0|\n",
      "|   MAWSON LAKES|      10.0|\n",
      "| ELIZABETH PARK|      10.0|\n",
      "|    HOLDEN HILL|       9.0|\n",
      "+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining Unit and Crash data using Outer join\n",
    "df_join = df_Crash.join(df_Units,df_Crash.REPORT_ID == df_Units.REPORT_ID, how = 'outer')\n",
    "# Selecting required columns 'Suburb', 'Total Cas', 'Licence Type'\n",
    "df_LicType = df_join.select(['Suburb', 'Total Cas', 'Licence Type'])\n",
    "# Filtering data based on Licence Type' equals to 'Unlicenced'\n",
    "df_UnLic = df_LicType[df_LicType['Licence Type'] == 'Unlicenced']\n",
    "# Calculating total casualties for each suburb\n",
    "df_CasualitiesSuburb = df_UnLic.groupby('Suburb').agg(F.sum('Total Cas').alias('casualties'))\n",
    "df_CasualitiesSuburb.sort('casualties', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Severity Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of crash events for each severity level. Which severity level is the\n",
    "most common?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|CSEF Severity|Total Severity|\n",
      "+-------------+--------------+\n",
      "|     4: Fatal|           451|\n",
      "|        2: MI|         21881|\n",
      "|       1: PDO|         46696|\n",
      "|        3: SI|          2978|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting total number of crash events for each severity level\n",
    "df_Severity = df_Crash.groupBy('CSEF Severity').agg(F.count('CSEF Severity').alias('Total Severity'))\n",
    "df_Severity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the total number of crash events for each severity level and the percentage\n",
    "for the four different scenarios.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------------+\n",
      "|CSEF Severity|Total_Crash|        Percentage|\n",
      "+-------------+-----------+------------------+\n",
      "|     4: Fatal|        451|0.6263366941643752|\n",
      "|        2: MI|      21881| 30.38774546565564|\n",
      "|       1: PDO|      46696|  64.8501513762742|\n",
      "|        3: SI|       2978| 4.135766463905786|\n",
      "+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting total number of crash events for each severity level\n",
    "df_SeverityCrash = df_Crash.groupBy('CSEF Severity').agg(F.count('CSEF Severity').alias('Total_Crash'))\n",
    "# Finding total crash events\n",
    "addition = df_SeverityCrash.groupBy().agg(F.sum(\"Total_Crash\")).collect()[0][0]\n",
    "# creating new column showing the percentage\n",
    "df_NewCol = df_SeverityCrash.withColumn('Percentage',(df_SeverityCrash.Total_Crash/addition)*100)\n",
    "\n",
    "df_NewCol.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the driver is tested positive on drugs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------------+\n",
      "|CSEF Severity|Positive on Drugs|        Percentage|\n",
      "+-------------+-----------------+------------------+\n",
      "|     4: Fatal|               82| 6.539074960127592|\n",
      "|        2: MI|              749|59.728867623604465|\n",
      "|       1: PDO|              176|14.035087719298245|\n",
      "|        3: SI|              247|19.696969696969695|\n",
      "+-------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering data when driver tested positive for drugs\n",
    "df_Drugs = df_Crash[df_Crash['Drugs Involved'] == 'Y']\n",
    "# Counting total number of crash events\n",
    "df_SeverityCrash = df_Drugs.groupBy('CSEF Severity').agg(F.count('Total Cas').alias('Positive on Drugs'))\n",
    "# Finding total crash events\n",
    "addition = df_SeverityCrash.groupBy().agg(F.sum(\"Positive on Drugs\")).collect()[0][0]\n",
    "# creating new column showing the percentage\n",
    "df_NewCol = df_SeverityCrash.withColumn('Percentage',(df_SeverityCrash[\"Positive on Drugs\"]/addition)*100)\n",
    "\n",
    "df_NewCol.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the driver is tested positive for blood alcohol concentration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+------------------+\n",
      "|CSEF Severity|Positive on Alcohol|        Percentage|\n",
      "+-------------+-------------------+------------------+\n",
      "|     4: Fatal|                 79|  3.51423487544484|\n",
      "|        2: MI|                737|  32.7846975088968|\n",
      "|       1: PDO|               1173|52.179715302491104|\n",
      "|        3: SI|                259| 11.52135231316726|\n",
      "+-------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering data when driver tested positive for alcohol\n",
    "df_Alcohol = df_Crash[df_Crash['DUI Involved'] == 'Y']\n",
    "# Counting total number of crash events\n",
    "df_SeverityCrash = df_Alcohol.groupBy('CSEF Severity').agg(F.count('Total Cas').alias('Positive on Alcohol'))\n",
    "# Finding total crash events\n",
    "addition = df_SeverityCrash.groupBy().agg(F.sum(\"Positive on Alcohol\")).collect()[0][0]\n",
    "# creating new column showing the percentage\n",
    "df_NewCol = df_SeverityCrash.withColumn('Percentage',(df_SeverityCrash[\"Positive on Alcohol\"]/addition)*100)\n",
    "\n",
    "df_NewCol.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the driver is tested positive for both drugs and blood alcohol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------+------------------+\n",
      "|CSEF Severity|Positive on Drugs/Alcohol|        Percentage|\n",
      "+-------------+-------------------------+------------------+\n",
      "|     4: Fatal|                       27|15.428571428571427|\n",
      "|        2: MI|                       89|50.857142857142854|\n",
      "|       1: PDO|                       24|13.714285714285715|\n",
      "|        3: SI|                       35|              20.0|\n",
      "+-------------+-------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering when driver is tested positive for both drugs and blood alcohol\n",
    "df_Alcohol_Drugs = df_Crash[(df_Crash['DUI Involved'] == 'Y') & (df_Crash['Drugs Involved'] == 'Y')]\n",
    "# Counting total number of crash events\n",
    "df_SeverityCrash = df_Alcohol_Drugs.groupBy('CSEF Severity').agg(F.count('Total Cas').alias('Positive on Drugs/Alcohol'))\n",
    "# Finding total crash events\n",
    "addition = df_SeverityCrash.groupBy().agg(F.sum(\"Positive on Drugs/Alcohol\")).collect()[0][0]\n",
    "# creating new column showing the percentage\n",
    "df_NewCol = df_SeverityCrash.withColumn('Percentage',(df_SeverityCrash[\"Positive on Drugs/Alcohol\"]/addition)*100)\n",
    "\n",
    "df_NewCol.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the driver is tested negative for both (no alcohol and no drugs).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------+------------------+\n",
      "|CSEF Severity|Negative on Drugs/Alcohol|        Percentage|\n",
      "+-------------+-------------------------+------------------+\n",
      "|     4: Fatal|                      317| 0.461567582521586|\n",
      "|        2: MI|                    20484|29.825710916000524|\n",
      "|       1: PDO|                    45371| 66.06240626683557|\n",
      "|        3: SI|                     2507|3.6503152346423215|\n",
      "+-------------+-------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering when driver is tested negative for both drugs and blood alcohol\n",
    "df_No_Alcohol_Drugs = df_Crash.filter((df_Crash['DUI Involved'].isNull()) & (df_Crash['Drugs Involved'].isNull()))\n",
    "# Counting total number of crash events\n",
    "df_SeverityCrash = df_No_Alcohol_Drugs.groupBy('CSEF Severity').agg(F.count('Total Cas').alias('Negative on Drugs/Alcohol'))\n",
    "# Finding total crash events\n",
    "addition = df_SeverityCrash.groupBy().agg(F.sum(\"Negative on Drugs/Alcohol\")).collect()[0][0]\n",
    "# creating new column showing the percentage\n",
    "df_NewCol = df_SeverityCrash.withColumn('Percentage',(df_SeverityCrash[\"Negative on Drugs/Alcohol\"]/addition)*100)\n",
    "\n",
    "df_NewCol.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 RDDs vs DataFrame vs Spark SQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the Date and Time of Crash, Number of Casualties in each unit and the Gender,\n",
    "Age, License Type of the unit driver for the suburb \"Adelaide\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 ms, sys: 4.29 ms, total: 47.3 ms\n",
      "Wall time: 4.63 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2016-November-Wednesday', '04:26 pm', '01', '0', 'Male', '017', 'Unknown'),\n",
       " ('2016-November-Wednesday', '04:26 pm', '02', '0', 'Male', '025', 'Unknown'),\n",
       " ('2016-December-Friday', '11:30 am', '01', '0', 'Male', '080', 'Full'),\n",
       " ('2016-December-Friday', '11:30 am', '02', '0', 'Male', '048', 'Full'),\n",
       " ('2016-December-Saturday', '07:40 am', '01', '0', 'Male', '032', 'Full'),\n",
       " ('2016-December-Saturday',\n",
       "  '07:40 am',\n",
       "  '02',\n",
       "  '0',\n",
       "  'Unknown',\n",
       "  'XXX',\n",
       "  'Unknown'),\n",
       " ('2016-December-Friday', '05:30 pm', '01', '0', 'Female', '058', 'Full'),\n",
       " ('2016-December-Friday', '05:30 pm', '02', '0', 'Male', '041', 'Full'),\n",
       " ('2016-December-Wednesday', '04:20 pm', '01', '0', 'Female', '045', 'Full'),\n",
       " ('2016-December-Wednesday', '04:20 pm', '02', '0', 'Male', '027', 'Full')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Creating key value pair based  on REPORT_ID\n",
    "def keyValue(li):\n",
    "    return (li[0], li[1:])\n",
    "\n",
    "Unit_keyValue = Unit_RDD1.map(keyValue)\n",
    "Crash_keyValue = Crash_RDD1.map(keyValue)\n",
    "\n",
    "# Joining Unit and Crash RDD based on Key REPORT_ID\n",
    "joined_RDD = Unit_keyValue.join(Crash_keyValue)\n",
    "# Filtering required columns for analysis\n",
    "RDD = joined_RDD.map(lambda x: (x[1][0][0],x[1][0][1],x[1][0][3],x[1][0][6],x[1][0][7],x[1][0][10],x[1][1][1],x[1][1][9],\n",
    "                                       x[1][1][10],x[1][1][11],x[1][1][12]))\n",
    "\n",
    "# Selecting columns Date, Time, Number of Casualties, Gender, Age, License Type for the suburb \"Adelaide\".\n",
    "NoOfCaus = RDD.filter(lambda x: x[6] == 'ADELAIDE').map(lambda x: (x[7]+\"-\"+x[8]+\"-\"+x[9],x[10],x[0],x[1],x[3],x[4],x[5]))\n",
    "NoOfCaus.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+---------+-------+----+------------+\n",
      "|                Date|    Time|Unit No|No Of Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+-------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|     02|        1|   Male| 072|        null|\n",
      "|2016-November-Wed...|01:45 pm|     01|        0|   Male| 056|        Full|\n",
      "|2016-November-Tue...|03:40 pm|     02|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|03:40 pm|     01|        0|   Male| 056|        null|\n",
      "|2016-November-Tue...|05:00 pm|     02|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:00 pm|     01|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:40 pm|     02|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|     01|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|     03|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:26 pm|     02|        0|   Male| 042|        Full|\n",
      "+--------------------+--------+-------+---------+-------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 6.93 ms, sys: 4.64 ms, total: 11.6 ms\n",
      "Wall time: 726 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# filtering data based on Suburb ADELAIDE\n",
    "df_Adelaide = df_join.filter(df_join['Suburb'] == 'ADELAIDE')\n",
    "\n",
    "x = \"-\" # character to join Year Month Day\n",
    "# Selecting columns Date, Time, Number of Casualties, Gender, Age, License Type for the suburb \"Adelaide\"\n",
    "df_Adelaide = df_Adelaide.select(F.concat('Year',F.lit(x),'Month',F.lit(x),'Day').alias('Date'),'Time','Unit No','No Of Cas','Sex','Age','Licence Type')\n",
    "\n",
    "df_Adelaide.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+---------+-------+---+------------+\n",
      "|                Date|    Time|Unit No|No Of Cas|    Sex|Age|Licence Type|\n",
      "+--------------------+--------+-------+---------+-------+---+------------+\n",
      "|2016-November-Wed...|01:45 pm|Unit No|No Of Cas|   Male|056|Licence Type|\n",
      "|2016-November-Wed...|01:45 pm|Unit No|No Of Cas|   Male|072|Licence Type|\n",
      "|2016-November-Tue...|03:40 pm|Unit No|No Of Cas|   Male|056|Licence Type|\n",
      "|2016-November-Tue...|03:40 pm|Unit No|No Of Cas| Female|027|Licence Type|\n",
      "|2016-November-Tue...|05:00 pm|Unit No|No Of Cas| Female|032|Licence Type|\n",
      "|2016-November-Tue...|05:00 pm|Unit No|No Of Cas|Unknown|XXX|Licence Type|\n",
      "|2016-November-Tue...|05:40 pm|Unit No|No Of Cas|   Male|022|Licence Type|\n",
      "|2016-November-Tue...|05:40 pm|Unit No|No Of Cas|   Male|020|Licence Type|\n",
      "|2016-November-Monday|11:26 pm|Unit No|No Of Cas|Unknown|XXX|Licence Type|\n",
      "|2016-November-Monday|11:26 pm|Unit No|No Of Cas|   Male|042|Licence Type|\n",
      "+--------------------+--------+-------+---------+-------+---+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 3.03 ms, sys: 343 µs, total: 3.37 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating Temporary view for both Unit and Crash Data\n",
    "df_Units.createOrReplaceTempView(\"UnitData\")\n",
    "df_Crash.createOrReplaceTempView(\"CrashData\")\n",
    "\n",
    "# Selecting columns Date, Time, Number of Casualties, Gender, Age, License Type for the suburb \"Adelaide\"\n",
    "SqlAdelaide = spark.sql('''\n",
    "SELECT CONCAT(Year, '-', Month, '-', Day) as Date, Time,'Unit No','No Of Cas', Sex, Age,'Licence Type'\n",
    "FROM UnitData u, CrashData c \n",
    "WHERE u.REPORT_ID = c.REPORT_ID and Suburb == 'ADELAIDE'\n",
    "''')\n",
    "\n",
    "SqlAdelaide.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of casualties for each suburb when the vehicle was driven by an\n",
    "unlicensed driver. You are required to display the name of the suburb and the total\n",
    "number of casualties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|         Suburb|Casualties|\n",
      "+---------------+----------+\n",
      "|       ADELAIDE|      19.0|\n",
      "|      DRY CREEK|      18.0|\n",
      "|      SALISBURY|      18.0|\n",
      "| SALISBURY EAST|      16.0|\n",
      "|       PROSPECT|      14.0|\n",
      "| NORTH ADELAIDE|      13.0|\n",
      "|        ENFIELD|      12.0|\n",
      "|   ANDREWS FARM|      12.0|\n",
      "|SALISBURY SOUTH|      11.0|\n",
      "|SALISBURY DOWNS|      11.0|\n",
      "+---------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 10.1 ms, sys: 156 µs, total: 10.3 ms\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Joining Unit and Crash data using Outer join\n",
    "df_join = df_Crash.join(df_Units,df_Crash.REPORT_ID == df_Units.REPORT_ID, how = 'outer')\n",
    "# Selecting required columns 'Suburb', 'Total Cas', 'Licence Type'\n",
    "df_LicType = df_join.select(['Suburb', 'Total Cas', 'Licence Type'])\n",
    "# Filtering data based on Licence Type' equals to 'Unlicenced'\n",
    "df_UnLic = df_LicType[df_LicType['Licence Type'] == 'Unlicenced']\n",
    "# Calculating total casualties for each suburb\n",
    "df_CasualitiesSuburb = df_UnLic.groupby('Suburb').agg(F.sum('Total Cas').alias('Casualties'))\n",
    "# displaying top 10 rows\n",
    "df_CasualitiesSuburb.sort('Casualties', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.5 ms, sys: 15.1 ms, total: 79.6 ms\n",
      "Wall time: 5.84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ADELAIDE', 19),\n",
       " ('KENT TOWN', 19),\n",
       " ('DRY CREEK', 18),\n",
       " ('SALISBURY', 18),\n",
       " ('PAYNEHAM', 17),\n",
       " ('NORWOOD', 17),\n",
       " ('SALISBURY EAST', 16),\n",
       " ('MARDEN', 16),\n",
       " ('STEPNEY', 15),\n",
       " ('MAYLANDS', 14)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Creating key value pair based  on REPORT_ID\n",
    "def keyValue(li):\n",
    "    return (li[0], li[1:])\n",
    "\n",
    "Unit_keyValue = Unit_RDD1.map(keyValue)\n",
    "Crash_keyValue = Crash_RDD1.map(keyValue)\n",
    "\n",
    "# Joining Unit and Crash RDD based on Key REPORT_ID\n",
    "joined_RDD = Unit_keyValue.join(Crash_keyValue)\n",
    "\n",
    "# Selecting required columns 'Suburb', 'Total Cas', 'Licence Type'\n",
    "RDD1 = joined_RDD.map(lambda x: (x[1][0][10],x[1][1][1],x[1][1][4],x[1][1][5]))\n",
    "# Filtering data based on Licence Type' equals to 'Unlicenced'\n",
    "RDD1 = RDD1.filter(lambda x: x[0]=='Unlicenced')\n",
    "# Converting data type for total cas to integer\n",
    "RDD1 = RDD1.map(lambda x: (x[1],int(x[3])))\n",
    "# Group by suburb and taking sum of Total Cas\n",
    "RDD1 = RDD1.groupByKey().mapValues(sum)\n",
    "# Sorting by key that is Suburb, top 10 rows\n",
    "RDD1.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|         Suburb|Casualties|\n",
      "+---------------+----------+\n",
      "|       ADELAIDE|      19.0|\n",
      "|      SALISBURY|      18.0|\n",
      "|      DRY CREEK|      18.0|\n",
      "| SALISBURY EAST|      16.0|\n",
      "|       PROSPECT|      14.0|\n",
      "| NORTH ADELAIDE|      13.0|\n",
      "|   ANDREWS FARM|      12.0|\n",
      "|        ENFIELD|      12.0|\n",
      "|SALISBURY DOWNS|      11.0|\n",
      "|     INGLE FARM|      11.0|\n",
      "+---------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 2.92 ms, sys: 249 µs, total: 3.17 ms\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Selecting columns 'Suburb', 'Total Cas', 'Licence Type' and group by Licence Type\n",
    "SqlCasualities = spark.sql('''\n",
    "SELECT Suburb, sum(`Total Cas`) as `Casualties` \n",
    "FROM UnitData u, CrashData c \n",
    "where u.REPORT_ID = c.REPORT_ID and `Licence Type` == \"Unlicenced\" \n",
    "GROUP BY Suburb ORDER BY `Casualties` DESC \n",
    "''')\n",
    "# displaying top 10 rows\n",
    "SqlCasualities.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
