{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 Part B<br>\n",
    "\n",
    "Name: Manmeet Singh<br>\n",
    "Id: 30749476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from datetime import timezone,datetime \n",
    "from itertools import chain\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "from pyspark.sql.functions import regexp_replace, concat\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "spark_conf = SparkConf().setMaster(\"local[2]\").setAppName(\"SparkStreaming\").set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading streaming data for process\n",
    "topic = \"process_producer\"\n",
    "process = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading streaming data for memory\n",
    "topic = \"memory_producer\"\n",
    "memory = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process schema\n",
    "process_type = ArrayType(StructType([\n",
    "    StructField(\"sequence\",IntegerType(),True),\n",
    "    StructField(\"machine\",IntegerType(),True),\n",
    "    StructField(\"PID\",IntegerType(),True),\n",
    "    StructField(\"TRUN\",IntegerType(),True),\n",
    "    StructField(\"TSLPI\",IntegerType(),True),\n",
    "    StructField(\"TSLPU\",IntegerType(),True),\n",
    "    StructField(\"POLI\",StringType(),True),\n",
    "    StructField(\"NICE\",IntegerType(),True),\n",
    "    StructField(\"PRI\",IntegerType(),True),\n",
    "    StructField(\"RTPR\",IntegerType(),True),\n",
    "    StructField(\"CPUNR\",IntegerType(),True),\n",
    "    StructField(\"Status\",StringType(),True),\n",
    "    StructField(\"EXC\",IntegerType(),True),\n",
    "    StructField(\"State\",StringType(),True),\n",
    "    StructField(\"CPU\",DoubleType(),True),\n",
    "    StructField(\"CMD\",StringType(),True),\n",
    "    StructField(\"ts\",TimestampType(),True)]))\n",
    "process = process.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory schema\n",
    "memory_type = ArrayType(StructType([\n",
    "    StructField(\"sequence\",IntegerType(),True),\n",
    "    StructField(\"machine\",IntegerType(),True),\n",
    "    StructField(\"PID\",DoubleType(),True),\n",
    "    StructField(\"MINFLT\",StringType(),True),\n",
    "    StructField(\"MAJFLT\",StringType(),True),\n",
    "    StructField(\"VSTEXT\",StringType(),True),\n",
    "    StructField(\"VSIZE\",DoubleType(),True),\n",
    "    StructField(\"RSIZE\",StringType(),True),\n",
    "    StructField(\"VGROW\",StringType(),True),\n",
    "    StructField(\"RGROW\",StringType(),True),\n",
    "    StructField(\"MEM\",DoubleType(),True),\n",
    "    StructField(\"CMD\",StringType(),True),\n",
    "    StructField(\"ts\",TimestampType(),True)]))\n",
    "memory = memory.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Process Values\n",
    "process = process.select(F.from_json(F.col(\"value\").cast(\"string\"),process_type).alias('parsed_value'))\n",
    "process = process.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Memory Values\n",
    "memory = memory.select(F.from_json(F.col(\"value\").cast(\"string\"),memory_type).alias('parsed_value'))\n",
    "memory = memory.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = process.select(\n",
    "    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "    F.col(\"unnested_value.TRUN\").alias(\"TRUN\"),\n",
    "    F.col(\"unnested_value.TSLPI\").alias(\"TSLPI\"),\n",
    "    F.col(\"unnested_value.TSLPU\").alias(\"TSLPU\"),\n",
    "    F.col(\"unnested_value.POLI\").alias(\"POLI\"),\n",
    "    F.col(\"unnested_value.NICE\").alias(\"NICE\"),\n",
    "    F.col(\"unnested_value.PRI\").alias(\"PRI\"),\n",
    "    F.col(\"unnested_value.RTPR\").alias(\"RTPR\"),\n",
    "    F.col(\"unnested_value.CPUNR\").alias(\"CPUNR\"),\n",
    "    F.col(\"unnested_value.Status\").alias(\"Status\"),\n",
    "    F.col(\"unnested_value.EXC\").alias(\"EXC\"),\n",
    "    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "    F.col(\"unnested_value.CPU\").alias(\"CPU\"),\n",
    "    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "    F.col(\"unnested_value.ts\").alias(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = memory.select(\n",
    "    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "    F.col(\"unnested_value.MINFLT\").alias(\"MINFLT\"),\n",
    "    F.col(\"unnested_value.MAJFLT\").alias(\"MAJFLT\"),\n",
    "    F.col(\"unnested_value.VSTEXT\").alias(\"VSTEXT\"),\n",
    "    F.col(\"unnested_value.VSIZE\").alias(\"VSIZE\"),\n",
    "    F.col(\"unnested_value.RSIZE\").alias(\"RSIZE\"),\n",
    "    F.col(\"unnested_value.VGROW\").alias(\"VGROW\"),\n",
    "    F.col(\"unnested_value.RGROW\").alias(\"RGROW\"),\n",
    "    F.col(\"unnested_value.MEM\").alias(\"MEM\"),\n",
    "    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "    F.col(\"unnested_value.ts\").alias(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring,length\n",
    "colList = ['MINFLT', 'MAJFLT', 'VSTEXT', 'RSIZE', 'VGROW', 'RGROW']\n",
    "\n",
    "for column in colList:\n",
    "    # trim space from numeric data\n",
    "    memory = memory.withColumn(column, F.trim(F.col(column)))\n",
    "    string = 'substring({0},1,length({0})-1)'.format(column)\n",
    "    # remove K from numeric data and multiply with 1000\n",
    "    memory = memory.withColumn(column,\\\n",
    "    F.when(F.col(column).contains('K'),F.expr(string).cast('int')*1000).otherwise(F.col(column).cast('int')))\n",
    "    # remove M from numeric data and multiply with 1000000\n",
    "    memory = memory.withColumn(column,\\\n",
    "    F.when(F.col(column).contains('M'),F.expr(string).cast('int')*1000000).otherwise(F.col(column).cast('int')))\n",
    "    \n",
    "process = process.withColumn(\"NICE\", F.when(F.col(\"PRI\")==0, 0).otherwise(F.col(\"PRI\")-120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. For process and memory, respectively, create a new column “CMD_PID”\n",
    "concatenating “CMD” and “PID” columns, and a new column “event_time” as\n",
    "timestamp format based on the unix time in “ts” column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New columns for process and memory data\n",
    "process = process.withColumn(\"CMD_PID\", concat(F.col('CMD'),F.col(\"PID\")))\n",
    "memory = memory.withColumn(\"event_time\",F.col('ts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Persist the transformed streaming data in parquet format for both process and\n",
    "memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet file for process\n",
    "query_file_sink_process = process.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"process.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"process.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet file for memory\n",
    "query_file_sink_memory = memory.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"memory.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"memory.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink_process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink_memory.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading process Parquet file\n",
    "query_file_sink_df_process = spark.read.parquet(\"process.parquet/\")\n",
    "query_file_sink_df_process.printSchema()\n",
    "query_file_sink_df_process.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading memory Parquet file\n",
    "query_file_sink_df_memory = spark.read.parquet(\"memory.parquet/\")\n",
    "query_file_sink_df_memory.printSchema()\n",
    "query_file_sink_df_memory.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Load the machine learning models given 6 , and use the models to predict whether\n",
    "each process or memory streaming record is an attack event, respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pipeline Model\n",
    "PipelineProcess = PipelineModel.load('process_pipeline_model')\n",
    "PipelineMemory = PipelineModel.load('memory_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "Process_transformed = PipelineProcess.transform(process)\n",
    "Memory_transformed = PipelineMemory.transform(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Using the prediction result, and monitor the data following the requirements below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track of the approximate count of such events in every 2-min window for each machine for process and memory\n",
    "windowedCounts_process = Process_transformed \\\n",
    "    .withWatermark(\"ts\", \"120 seconds\") \\\n",
    "    .where('prediction==1')\\\n",
    "    .groupBy(F.window(process.ts, \"120 seconds\"),F.col('machine'))\\\n",
    "    .agg(F.approx_count_distinct(\"CMD_PID\").alias(\"total\"))\\\n",
    "    .select(\"window\",\"machine\",\"total\")\n",
    "\n",
    "windowedCounts_memory = Memory_transformed \\\n",
    "    .withWatermark(\"ts\", \"120 seconds\") \\\n",
    "    .where('prediction==1')\\\n",
    "    .groupBy(F.window(memory.ts, \"120 seconds\"),F.col('machine'))\\\n",
    "    .agg(F.count(\"machine\").alias(\"total\"))\\\n",
    "    .select(\"window\",\"machine\",\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_process = windowedCounts_process \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()\n",
    "\n",
    "query_memory = windowedCounts_memory \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_memory.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns before joining, Process\n",
    "processFormatted = Process_transformed\\\n",
    "            .selectExpr(\"sequence as processSequence\",\"machine as processMachine\", \"CMD as CMD_process\"\\\n",
    "                        ,\"PID AS PID_process\",\"TRUN\",\"TSLPI\",\"TSLPU\",\"POLI\",\"NICE\"\\\n",
    "                       ,\"PRI\",\"RTPR\",\"CPUNR\",\"Status\",\"EXC\",\"State\",\"CPU\",\"ts as process_ts\",\"CMD_PID\"\\\n",
    "                        ,\"prediction as processPrediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns before joining, Memory\n",
    "memoryFormatted = Memory_transformed\\\n",
    "            .selectExpr(\"sequence as memorySequence\",\"machine as memoryMachine\", \"CMD as CMD_memory\",\"PID as PID_memory\"\\\n",
    "                        ,\"MINFLT\",\"MAJFLT\",\"VSTEXT\",\"VSIZE\",\"RSIZE\",\"VGROW\",\"RGROW\",\"MEM\"\\\n",
    "                        ,\"ts as memory_ts\",\"event_time\",'prediction as memoryPrediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging process and memory data where attack is 1\n",
    "data_merged = processFormatted.join(memoryFormatted,expr(\"\"\"\n",
    "    CMD_process = CMD_memory AND PID_process = PID_memory AND\n",
    "    processPrediction = 1 AND\n",
    "    memoryPrediction = 1 AND\n",
    "    process_ts <= memory_ts + interval 30 seconds AND memory_ts <= process_ts + interval 30 seconds\n",
    "\"\"\"))\n",
    "data_merged = data_merged.withColumn(\"processingTime\",F.lit(int(datetime.now().replace(tzinfo=timezone.utc).timestamp())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet file for merged data\n",
    "attack_data = data_merged.select(\"processSequence\",\"processMachine\"\\\n",
    "                                 ,\"CMD_process\",\"PID_process\"\\\n",
    "                                 ,\"TRUN\",\"TSLPI\",\"TSLPU\",\"POLI\",\"NICE\"\\\n",
    "                                 ,\"PRI\",\"RTPR\",\"CPUNR\",\"Status\",\"EXC\",\"State\",\"CPU\"\\\n",
    "                                 ,\"process_ts\",\"processPrediction\"\\\n",
    "                                 ,\"memorySequence\",\"memoryMachine\"\\\n",
    "                                 ,\"CMD_memory\",\"PID_memory\"\\\n",
    "                                 ,\"MAJFLT\",\"VSTEXT\",\"VSIZE\",\"RSIZE\",\"VGROW\",\"RGROW\",\"MEM\"\\\n",
    "                                 ,\"memory_ts\",\"event_time\",\"memoryPrediction\",\"processingTime\")\\\n",
    "        .writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"process_memory_attack.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"process_memory_attack.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_data.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Parquet file\n",
    "query_file_sink_attack_data = spark.read.parquet(\"process_memory_attack.parquet/\")\n",
    "query_file_sink_attack_data.printSchema()\n",
    "query_file_sink_attack_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
